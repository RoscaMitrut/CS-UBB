{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import random\n",
    "import urllib\n",
    "import re\n",
    "#scipy <= 1.12\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/corpus2.txt\",encoding=\"utf-8\") as f:\n",
    "\ttext = f.read()\n",
    "\n",
    "with open(\"data/corpus_complet.txt\", encoding=\"utf-8\") as f:\n",
    "\ttext_ro = f.read()\n",
    "\n",
    "text_model = markovify.Text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this, Give warning to the sweet love even by yours, "
     ]
    }
   ],
   "source": [
    "def mar(text):\n",
    "\t#TODO: REGEX PENTRU PUNCTUATIE\n",
    "\twords = text.split()\n",
    "\tdict_out = dict()\n",
    "\tfor i in range(len(words)-1):\n",
    "\t\tdict_out.setdefault(words[i],[]).append(words[i+1])\n",
    "\treturn dict_out\n",
    "\n",
    "dictionar = mar(text)\n",
    "\n",
    "def generate(dictionary):\n",
    "\tprop = []\n",
    "\tr1,r2 = random.choice(list(dictionary.items()))\n",
    "\tcurrent = dictionary[r1][0]\n",
    "\tprop.append(current)\n",
    "\tfor _ in range(10):\n",
    "\t\tmax_len = len(dictionary[current])\n",
    "\t\trandom_int = random.randint(0,max_len-1)\n",
    "\t\tcurrent = dictionary[current][random_int]\n",
    "\t\tprop.append(current)\n",
    "\treturn prop\t\n",
    "\n",
    "ceva = generate(dictionar)\n",
    "for i in range(len(ceva)):\n",
    "\tprint(ceva[i],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plânge... el palid se pierde în negurile reci, De mă găsesc iar "
     ]
    }
   ],
   "source": [
    "def mar_n(text,n=1):\n",
    "\twords = text.split()\n",
    "\tdict_out = dict()\n",
    "\tfor i in range(len(words)-(n)):#n+1 ?\n",
    "\t\tcheie = []\n",
    "\t\tfor j in range(n):\n",
    "\t\t\tcheie.append(words[i+j])\n",
    "\t\tdict_out.setdefault(' '.join(cheie),[]).append(words[i+n])\n",
    "\treturn dict_out\n",
    "\n",
    "dictionar3 = mar_n(text_ro,2)\t\n",
    "\n",
    "def generate_n(dictionary):\n",
    "\tprop = []\n",
    "\tcurrent,r2 = random.choice(list(dictionary.items()))\n",
    "\tprop.append(current)\n",
    "\tfor _ in range(10):\n",
    "\t\tmax_len = len(dictionary[current])\n",
    "\t\trandom_int = random.randint(0,max_len-1)\n",
    "\t\tcurrent = current +' '+ dictionary[current][random_int]\n",
    "\t\tcurrent = current.split(' ',1)[1]\n",
    "\t\tprop.append(current.split()[-1])\n",
    "\treturn prop\t\n",
    "\n",
    "ceva3 = generate_n(dictionar3)\n",
    "for i in range(len(ceva3)):\n",
    "\tprint(ceva3[i],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou art as tyrannous, so as foes commend.\n",
      "When in the carcanet.\n",
      "They know what beauty was of yore.\n",
      "Is't not enough to cure me.\n",
      "Oh that record could with a perpetual dulness.\n",
      "Look what is told.\n",
      "Some glory in their badness reign.\n",
      "O! let me, true in love, to thee resort.\n",
      "Look what is told.\n",
      "I love her; And for this sin there is no need.\n"
     ]
    }
   ],
   "source": [
    "nr_versuri = 10\n",
    "\n",
    "def generate():\n",
    "\ttry:\n",
    "\t\tversuri = [text_model.make_short_sentence(50) for i in range(nr_versuri)]\n",
    "\t\tpoezie = \"\\n\".join(versuri)\n",
    "\t\treturn poezie\n",
    "\texcept TypeError:\n",
    "\t\treturn generate()\n",
    "\n",
    "poezie = generate()\n",
    "\n",
    "print(poezie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "\t# Initialize the sentiment analyzer\n",
    "\tsid = SentimentIntensityAnalyzer()\n",
    "\t\n",
    "\t# Analyze sentiment\n",
    "\tsentiment_scores = sid.polarity_scores(text)\n",
    "\t\n",
    "\t# Print sentiment scores\n",
    "\tprint(\"Sentiment Analysis Results:\")\n",
    "\tprint(\"Positive:\", sentiment_scores['pos'])\n",
    "\tprint(\"Negative:\", sentiment_scores['neg'])\n",
    "\tprint(\"Neutral:\", sentiment_scores['neu'])\n",
    "\tprint(\"Compound:\", sentiment_scores['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results:\n",
      "Positive: 0.25\n",
      "Negative: 0.111\n",
      "Neutral: 0.639\n",
      "Compound: 0.9221\n"
     ]
    }
   ],
   "source": [
    "analyze_sentiment(poezie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/alexisperrier/intro2nlp/master/data/Shakespeare_alllines.txt'\n",
    "\n",
    "lines = urllib.request.urlopen(url).read().decode('utf-8').split(\"\\n\")\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for line in lines:\n",
    "\t# remove punctuation\n",
    "\tline = re.sub(r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]','',line).strip()\n",
    "\n",
    "\ttokens = re.findall(r'\\b\\w+\\b', line)\n",
    "\n",
    "\tif len(tokens) > 1:\n",
    "\t\tsentences.append(tokens)\n",
    "\n",
    "bard2vec = Word2Vec(\n",
    "\t\t\tsentences,\n",
    "\t\t\tmin_count=2,   # Ignore words that appear less than this\n",
    "\t\t\tvector_size=50,       # Dimensionality of word embeddings\n",
    "\t\t\tsg = 1,        # skipgrams\n",
    "\t\t\twindow=7,      # Context window for words during training\n",
    "\t\t\tepochs=40)       # Number of epochs training over corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "poezie_sinonima = []\n",
    "\n",
    "for vers in poezie.split(\"\\n\"):\n",
    "\tvers_sinonim = \"\"\n",
    "\tfor cuvant in vers.split():\n",
    "\t\ttry:\n",
    "\t\t\tif random.randint(1,2) == 2:\n",
    "\t\t\t\traise Exception\n",
    "\t\t\tsinonim = bard2vec.wv.most_similar(cuvant)\n",
    "\t\t\tvers_sinonim += sinonim[0][0] + \" \"\n",
    "\t\texcept:\n",
    "\t\t\tvers_sinonim += cuvant+\" \"\n",
    "\tpoezie_sinonima.append(vers_sinonim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou thou as tyrannous, too as parents commend. \n",
      "That of this carcanet. \n",
      "They understand what virtue was fundamental yore. \n",
      "Is't slowness reeling Boskos hiding me. \n",
      "Oh which record could and vendible entail dulness. \n",
      "Look What tis told. \n",
      "Some glory of their badness reign. \n",
      "O! let me, mistrusted in love, Boskos me resort. \n",
      "See what tis told. \n",
      "you hate her; And for Sport sin there is none need. \n"
     ]
    }
   ],
   "source": [
    "for elem in poezie_sinonima:\n",
    "\tprint(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "poezie_sinonima_split = []\n",
    "for el in poezie_sinonima:\n",
    "    poezie_sinonima_split.append(el.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Thou', 'thou', 'as', 'tyrannous,', 'too', 'as', 'parents', 'commend.'],\n",
       " ['That', 'of', 'this', 'carcanet.'],\n",
       " ['They', 'understand', 'what', 'virtue', 'was', 'fundamental', 'yore.'],\n",
       " [\"Is't\", 'slowness', 'reeling', 'Boskos', 'hiding', 'me.'],\n",
       " ['Oh', 'which', 'record', 'could', 'and', 'vendible', 'entail', 'dulness.'],\n",
       " ['Look', 'What', 'tis', 'told.'],\n",
       " ['Some', 'glory', 'of', 'their', 'badness', 'reign.'],\n",
       " ['O!', 'let', 'me,', 'mistrusted', 'in', 'love,', 'Boskos', 'me', 'resort.'],\n",
       " ['See', 'what', 'tis', 'told.'],\n",
       " ['you',\n",
       "  'hate',\n",
       "  'her;',\n",
       "  'And',\n",
       "  'for',\n",
       "  'Sport',\n",
       "  'sin',\n",
       "  'there',\n",
       "  'is',\n",
       "  'none',\n",
       "  'need.']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poezie_sinonima_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Thou', 'art', 'as', 'tyrannous,', 'so', 'as', 'foes', 'commend.'],\n",
       " ['When', 'in', 'the', 'carcanet.'],\n",
       " ['They', 'know', 'what', 'beauty', 'was', 'of', 'yore.'],\n",
       " [\"Is't\", 'not', 'enough', 'to', 'cure', 'me.'],\n",
       " ['Oh', 'that', 'record', 'could', 'with', 'a', 'perpetual', 'dulness.'],\n",
       " ['Look', 'what', 'is', 'told.'],\n",
       " ['Some', 'glory', 'in', 'their', 'badness', 'reign.'],\n",
       " ['O!', 'let', 'me,', 'true', 'in', 'love,', 'to', 'thee', 'resort.'],\n",
       " ['Look', 'what', 'is', 'told.'],\n",
       " ['I',\n",
       "  'love',\n",
       "  'her;',\n",
       "  'And',\n",
       "  'for',\n",
       "  'this',\n",
       "  'sin',\n",
       "  'there',\n",
       "  'is',\n",
       "  'no',\n",
       "  'need.']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poezie_split1 = []\n",
    "poezie_split1 = poezie.split(\"\\n\")\n",
    "\n",
    "poezie_split = []\n",
    "for el in poezie_split1:\n",
    "    poezie_split.append(el.split())\n",
    "\n",
    "poezie_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score:  8.15394372921771e-155\n",
      "BLEU Score:  1.6954057018456463e-231\n",
      "BLEU Score:  1.583976781977924e-231\n",
      "BLEU Score:  1.384292958842266e-231\n",
      "BLEU Score:  7.711523862191631e-155\n",
      "BLEU Score:  1.5319719891192393e-231\n",
      "BLEU Score:  7.600793306264725e-78\n",
      "BLEU Score:  5.30941411456236e-78\n",
      "BLEU Score:  1.5319719891192393e-231\n",
      "BLEU Score:  5.956269172906508e-78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoscaMitrut\\.conda\\envs\\tfgpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\RoscaMitrut\\.conda\\envs\\tfgpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\RoscaMitrut\\.conda\\envs\\tfgpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "for i in range(len(poezie_split)):\n",
    "\tbleu_score = sentence_bleu(poezie_split, poezie_sinonima_split[i])\n",
    "\tprint(\"BLEU Score: \", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Thou', 'art', 'as', 'tyrannous,', 'so', 'as', 'foes', 'commend.'], ['When', 'in', 'the', 'carcanet.'], ['They', 'know', 'what', 'beauty', 'was', 'of', 'yore.'], [\"Is't\", 'not', 'enough', 'to', 'cure', 'me.'], ['Oh', 'that', 'record', 'could', 'with', 'a', 'perpetual', 'dulness.'], ['Look', 'what', 'is', 'told.'], ['Some', 'glory', 'in', 'their', 'badness', 'reign.'], ['O!', 'let', 'me,', 'true', 'in', 'love,', 'to', 'thee', 'resort.'], ['Look', 'what', 'is', 'told.'], ['I', 'love', 'her;', 'And', 'for', 'this', 'sin', 'there', 'is', 'no', 'need.']] \n",
      "\n",
      "['Thou', 'art', 'as', 'tyrannous,', 'so', 'as', 'foes', 'commend.'] ['Thou', 'thou', 'as', 'tyrannous,', 'too', 'as', 'parents', 'commend.']\n",
      "BLEU Score:  8.15394372921771e-155\n",
      "\n",
      "['When', 'in', 'the', 'carcanet.'] ['That', 'of', 'this', 'carcanet.']\n",
      "BLEU Score:  1.6954057018456463e-231\n",
      "\n",
      "['They', 'know', 'what', 'beauty', 'was', 'of', 'yore.'] ['They', 'understand', 'what', 'virtue', 'was', 'fundamental', 'yore.']\n",
      "BLEU Score:  1.583976781977924e-231\n",
      "\n",
      "[\"Is't\", 'not', 'enough', 'to', 'cure', 'me.'] [\"Is't\", 'slowness', 'reeling', 'Boskos', 'hiding', 'me.']\n",
      "BLEU Score:  1.384292958842266e-231\n",
      "\n",
      "['Oh', 'that', 'record', 'could', 'with', 'a', 'perpetual', 'dulness.'] ['Oh', 'which', 'record', 'could', 'and', 'vendible', 'entail', 'dulness.']\n",
      "BLEU Score:  7.711523862191631e-155\n",
      "\n",
      "['Look', 'what', 'is', 'told.'] ['Look', 'What', 'tis', 'told.']\n",
      "BLEU Score:  1.5319719891192393e-231\n",
      "\n",
      "['Some', 'glory', 'in', 'their', 'badness', 'reign.'] ['Some', 'glory', 'of', 'their', 'badness', 'reign.']\n",
      "BLEU Score:  7.600793306264725e-78\n",
      "\n",
      "['O!', 'let', 'me,', 'true', 'in', 'love,', 'to', 'thee', 'resort.'] ['O!', 'let', 'me,', 'mistrusted', 'in', 'love,', 'Boskos', 'me', 'resort.']\n",
      "BLEU Score:  5.30941411456236e-78\n",
      "\n",
      "['Look', 'what', 'is', 'told.'] ['See', 'what', 'tis', 'told.']\n",
      "BLEU Score:  1.5319719891192393e-231\n",
      "\n",
      "['I', 'love', 'her;', 'And', 'for', 'this', 'sin', 'there', 'is', 'no', 'need.'] ['you', 'hate', 'her;', 'And', 'for', 'Sport', 'sin', 'there', 'is', 'none', 'need.']\n",
      "BLEU Score:  5.956269172906508e-78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(poezie_split,\"\\n\")\n",
    "for i in range(len(poezie_split)):\n",
    "\tprint(poezie_split[i],poezie_sinonima_split[i])\n",
    "\tbleu_score = sentence_bleu(poezie_split, poezie_sinonima_split[i])\n",
    "\tprint(\"BLEU Score: \", bleu_score)\n",
    "\tprint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
